gains = df$gain,
losses = df$loss,
choices = df$choice,
method = "L-BFGS-B",
lower = c(0.01, 0.01),
upper = c(5, 10)
)
# Likelihood ratio test
D = -2 * (fit2$value - fit1$value)
p = pchisq(D, df=1, lower.tail=FALSE)
tibble(
ll_model1 = -fit1$value,
ll_model2 = -fit2$value,
lambda_est = fit1$par[1],
rho1 = fit1$par[2],
mu1 = fit1$par[3],
rho2 = fit2$par[1],
mu2 = fit2$par[2],
LR_D = D,
LR_p = p
)
})
dftmp = choices[choices$participant == '020',]
dftmp
dftmp = choices[choices$participant == '020',]
start1 <- c(lambda=1.2, rho=0.8, mu=1)
# Model 1 fit
fit1 <- optim(
par = start1,
fn = nll_model1,
gains = dftmp$gain,
losses = dftmp$loss,
choices = dftmp$choice,
method = "L-BFGS-B",
lower = c(0.01, 0.01, 0.01),
upper = c(5, 5, 10)
)
dftmp
dftmp = choices[choices$participant == '020',]
dftmp2 = filter(!is.na(dftmp$choice), !is.na(dftmp$gain), !is.na(dftmp$loss))
dftmp2 = dplyr::filter(!is.na(dftmp$choice), !is.na(dftmp$gain), !is.na(dftmp$loss))
dftmp = choices[choices$participant == '020',]
dftmp2 = dplyr::filter(dftmp, !is.na(choice), !is.na(gain), !is.na(loss))
dftmp2
# Model 1 fit
fit1 <- optim(
par = start1,
fn = nll_model1,
gains = dftmp2$gain,
losses = dftmp2$loss,
choices = dftmp2$choice,
method = "L-BFGS-B",
lower = c(0.01, 0.01, 0.01),
upper = c(5, 5, 10)
)
View(dftmp2)
# Negative log-likelihood function for Model 1 (lambda estimated)
nll_model1 <- function(params, gains, losses, choices) {
lambda <- params[1]
rho <- params[2]
mu <- params[3]
# 1) compute utilities (example)
U_gain <- gains^lambda
U_loss <- -mu * (abs(losses)^lambda)   # now 0^λ = 0, negative^λ never occurs
V       <- U_gain + rho * U_loss
p_raw <- 1 / (1 + exp(-V))
eps   <- .Machine$double.eps
p     <- pmin(pmax(p_raw, eps), 1 - eps)
if (any(is.na(choices))) {
stop("Missing values in choices!")
}
ll <- ifelse(choices == 1, log(p), log(1 - p))
return(-sum(ll))
}
dftmp = choices[choices$participant == '020',]
dftmp2 = dplyr::filter(dftmp, !is.na(choice), !is.na(gain), !is.na(loss))
start1 <- c(lambda=1.2, rho=0.8, mu=1)
# Model 1 fit
fit1 <- optim(
par = start1,
fn = nll_model1,
gains = dftmp2$gain,
losses = dftmp2$loss,
choices = dftmp2$choice,
method = "L-BFGS-B",
lower = c(0.01, 0.01, 0.01),
upper = c(5, 5, 10)
)
fit1
nll_model2 <- function(params, gains, losses, choices) {
# fixed exponent
lambda <- 1
# free parameters
rho <- params[1]      # loss weight
mu  <- params[2]      # decision "temperature"
# 1) compute utilities
U_gain <- gains^lambda
U_loss <- -rho * (abs(losses)^lambda)   # flip‐sign & abs‐before‐power
V <- U_gain + U_loss
# 2) choice probability
p_raw     <- 1 / (1 + exp(-mu * V))
# 3) clamp to avoid log(0)
eps       <- .Machine$double.eps
p_accept  <- pmin(pmax(p_raw, eps), 1 - eps)
# 4) negative log‐likelihood
if (any(is.na(choices))) stop("Missing choices!")
nll       <- -sum(ifelse(choices == 1,
log(p_accept),
log(1 - p_accept)))
return(nll)
}
# Fit both models per participant
results <- choices %>%
group_by(participant) %>%
group_modify(~{
df <- .x %>%
filter(!is.na(choice), !is.na(gain), !is.na(loss))
# Model 1 starting values
start1 <- c(lambda=1.2, rho=0.8, mu=1)
# Model 2 starting values
start2 <- c(rho=0.8, mu=1)
# Model 1 fit
fit1 <- optim(
par = start1,
fn = nll_model1,
gains = df$gain,
losses = df$loss,
choices = df$choice,
method = "L-BFGS-B",
lower = c(0.01, 0.01, 0.01),
upper = c(5, 5, 10)
)
# Model 2 fit
fit2 <- optim(
par = start2,
fn = nll_model2,
gains = df$gain,
losses = df$loss,
choices = df$choice,
method = "L-BFGS-B",
lower = c(0.01, 0.01),
upper = c(5, 10)
)
# Likelihood ratio test
D = -2 * (fit2$value - fit1$value)
p = pchisq(D, df=1, lower.tail=FALSE)
tibble(
ll_model1 = -fit1$value,
ll_model2 = -fit2$value,
lambda_est = fit1$par[1],
rho1 = fit1$par[2],
mu1 = fit1$par[3],
rho2 = fit2$par[1],
mu2 = fit2$par[2],
LR_D = D,
LR_p = p
)
})
# View results
print(results)
LR_D = (estimated_nlls_restricted - estimated_nlls)
# Set up the parallelization
n.cores <- parallel::detectCores() - 1; # Use 1 less than the full number of cores.
my.cluster <- parallel::makeCluster(
n.cores,
type = "FORK"
)
n.cores <- detectCores(logical = TRUE) - 1
library(parallel)
n.cores <- detectCores(logical = TRUE) - 1
cl <- makeCluster(n.cores, type = "PSOCK")
cd('C:\Users\Admin\Downloads\clasedecisiontask-main\clasedecisiontask-main\analysis\behavior')
setwd('C:\Users\Admin\Downloads\clasedecisiontask-main\clasedecisiontask-main\analysis\behavior')
setwd('C:\\Users\\Admin\\Downloads\\clasedecisiontask-main\\clasedecisiontask-main\\analysis\\behavior')
library('ggplot2')
library('doParallel')
library('foreach')
library('numDeriv')
library('here')
library('rstan')
library('lme4')
library('lmerTest')
# Working directory needs to be set to `parameter_recovery` directory of the repository.
#setwd('/Users/sokolhessner/Documents/gitrepos/clasedecisiontask/analysis/behavior/')
setwd('C:\\Users\\Admin\\Downloads\\clasedecisiontask-main\\clasedecisiontask-main\\analysis\\behavior')
source('./choice_probability.R');
source('./negLLprospect.R');
source('./negLLprospect_lambda1.R');
source('./check_trial_analysis.R');
eps = .Machine$double.eps;
iterations_per_estimation = 200; # how many times to perform the maximum likelihood estimation procedure on a given choiceset, for robustness.
# Configure this for the system it's being used on
datapath = '~/Documents/Dropbox/Academics/Research/CLASE Project 2021/data/Behavioral data files/'; # DATA PATH FOR PSH
fn = dir(datapath,pattern = glob2rx('clase*csv'),full.names = T);
number_of_subjects = length(fn)
number_of_subjects = length(fn)
#### Get the data ####
data = as.data.frame(matrix(data = NA, nrow = 0, ncol = 14))
for(i in 1:number_of_subjects){
tmpf = read.csv(fn[i]);
data = rbind(data,tmpf)
}
#### Get the data ####
data = as.data.frame(matrix(data = NA, nrow = 0, ncol = 14))
for(i in 1:number_of_subjects){
tmpf = read.csv(fn[i]);
data = rbind(data,tmpf)
}
# Configure this for the system it's being used on
#datapath = '~/Documents/Dropbox/Academics/Research/CLASE Project 2021/data/Behavioral data files/'; # DATA PATH FOR PSH
datapath = "D:\\Dropbox\\CLASE Project 2021\\data\\Behavioral data files"
fn = dir(datapath,pattern = glob2rx('clase*csv'),full.names = T);
number_of_subjects = length(fn)
#### Get the data ####
data = as.data.frame(matrix(data = NA, nrow = 0, ncol = 14))
for(i in 1:number_of_subjects){
tmpf = read.csv(fn[i]);
data = rbind(data,tmpf)
}
subjIDs = unique(data$subjID);
likelihood_correct_check_trial = array(dim = c(number_of_subjects,1));
# LFP Data path
lfpdatapath = '~/Documents/Dropbox/Academics/Research/CLASE Project 2021/R01-2024_02_05-Submission/NeuroBehaviorPreliminary/';
data[,'otc_loss'] = NA
data[,'otc_gain'] = NA
data[data$outcome == data$riskyloss,'otc_loss'] = data$outcome[data$outcome == data$riskyloss];
data[data$outcome == data$riskygain,'otc_gain'] = data$outcome[data$outcome == data$riskygain];
data[,'otc_loss'] = NA
data[,'otc_gain'] = NA
data[data$outcome == data$riskyloss,'otc_loss'] = data$outcome[data$outcome == data$riskyloss];
View(data)
View(data)
View(data)
library('ggplot2')
library('doParallel')
library('foreach')
library('numDeriv')
library('here')
library('rstan')
# Working directory needs to be set to `parameter_recovery` directory of the repository.
setwd('C:\\Users\\Admin\\Documents\\Github\\LossAversionManuscript')
source('./choice_probability.R');
source('./negLLprospect.R');
source('./check_trial_analysis.R');
source('./negLLprospect_lambda1.R')
source('./choice_probability_lambda1.R')
eps = .Machine$double.eps;
iterations_per_estimation = 200; # how many times to perform the maximum likelihood estimation procedure on a given choiceset, for robustness.
# Configure this for the system it's being used on
datapath = 'D:\\Dropbox\\CLASE Project 2021\\data\\Behavioral data files'; # DATA PATH FOR PSH
fn = dir(datapath,pattern = glob2rx('clase*csv'),full.names = T);
number_of_subjects = length(fn)
#### Get the data ####
data = as.data.frame(matrix(data = NA, nrow = 0, ncol = 14))
for(i in 1:number_of_subjects){
tmpf = read.csv(fn[i]);
data = rbind(data,tmpf)
}
subjIDs = unique(data$subjID);
likelihood_correct_check_trial = array(dim = c(number_of_subjects,1));
#### Initialize estimation procedure ####
set.seed(Sys.time()); # Estimation procedure is sensitive to starting values
number_of_parameters = 3;
initial_values_lowerbound = c(0.6, 0.2, 25); # for rho, lambda, and mu
initial_values_upperbound = c(1.4, 5, 100) - initial_values_lowerbound; # for rho, lambda, and mu
estimation_lowerbound = c(eps,eps,eps); # lower bound of parameter values is machine precision above zero
estimation_upperbound = c(2, 8, 300); # sensible/probable upper bounds on parameter values
# Create placeholders for the final estimates of the parameters, errors, and NLLs
estimated_parameters = array(dim = c(number_of_subjects, number_of_parameters),
dimnames = list(c(), c('rho','lambda','mu')));
estimated_parameter_errors = array(dim = c(number_of_subjects, number_of_parameters),
dimnames = list(c(), c('rho','lambda','mu')));
estimated_nlls = array(dim = c(number_of_subjects,1));
mean_choice_likelihood = array(dim = c(number_of_subjects,1));
mean_reactiontimes = array(dim = c(number_of_subjects,1));
mean_riskychoices = array(dim = c(number_of_subjects,1));
# Set up the parallelization
n.cores <- parallel::detectCores() - 1; # Use 1 less than the full number of cores.
my.cluster <- parallel::makeCluster(
n.cores,
type = "FORK"
)
doParallel::registerDoParallel(cl = my.cluster)
estimation_start_time = proc.time()[[3]]; # Start the clock
for (subject in 1:number_of_subjects){
subject_data = data[data$subjID == subjIDs[subject],];
finite_ind = is.finite(subject_data$choice);
tmpdata <- subject_data[finite_ind,]; # remove rows with NAs (missed choices) from estimation
choiceset = tmpdata[, 1:3];
choices = tmpdata$choice;
number_check_trials = sum(tmpdata$ischecktrial);
likelihood_correct_check_trial[subject] = check_trial_analysis(tmpdata);
mean_reactiontimes[subject] = mean(tmpdata$RT);
mean_riskychoices[subject] = mean(tmpdata$choice);
# Placeholders for all the iterations of estimation we're doing
all_estimates = matrix(nrow = iterations_per_estimation, ncol = number_of_parameters);
all_nlls = matrix(nrow = iterations_per_estimation, ncol = 1);
all_hessians = array(dim = c(iterations_per_estimation, number_of_parameters, number_of_parameters))
# The parallelized loop
alloutput <- foreach(iteration=1:iterations_per_estimation, .combine=rbind) %dopar% {
initial_values = runif(3)*initial_values_upperbound + initial_values_lowerbound; # create random initial values
# The estimation itself
output <- optim(initial_values, negLLprospect, choiceset = choiceset, choices = choices,
method = "L-BFGS-B", lower = estimation_lowerbound, upper = estimation_upperbound, hessian = TRUE);
c(output$par,output$value); # the things (parameter values & NLL) to save/combine across parallel estimations
}
all_estimates = alloutput[,1:3];
all_nlls = alloutput[,4];
best_nll_index = which.min(all_nlls); # identify the single best estimation
# Save out the parameters & NLLs from the single best estimation
estimated_parameters[subject,] = all_estimates[best_nll_index,];
estimated_nlls[subject] = all_nlls[best_nll_index];
# Calculate & store the mean choice likelihood given our best estimates
choiceP = choice_probability(choiceset,estimated_parameters[subject,]);
mean_choice_likelihood[subject] = mean(choices * choiceP + (1 - choices) * (1-choiceP));
# Calculate the hessian at those parameter values & save out
best_hessian = hessian(func=negLLprospect, x = all_estimates[best_nll_index,], choiceset = choiceset, choices = choices)
estimated_parameter_errors[subject,] = sqrt(diag(solve(best_hessian)));
binary_gainloss_plot = ggplot(data = tmpdata[tmpdata$riskyloss < 0,], aes(x = riskygain, y = riskyloss)) +
geom_point(aes(color = as.logical(tmpdata$choice[tmpdata$riskyloss < 0]), alpha = 0.7, size = 3)) +
scale_color_manual(values = c('#ff0000','#00ff44'), guide='none') +
theme_linedraw() + theme(legend.position = "none", aspect.ratio=1) +
ggtitle(sprintf('Gain-Loss Decisions: CLASE%03g',subjIDs[subject]));
print(binary_gainloss_plot);
fig_name = sprintf('gainloss_CLASE%03g.png',subjIDs[subject]);
if (!file.exists(fig_name)){
ggsave(fig_name,height=4.2,width=4.6,dpi=300);
}
binary_gainonly_plot = ggplot(data = tmpdata[tmpdata$riskyloss >= 0,], aes(x = riskygain, y = certainalternative)) +
geom_point(aes(color = as.logical(tmpdata$choice[tmpdata$riskyloss >= 0]), alpha = 0.7, size = 3)) +
scale_color_manual(values = c('#ff0000','#00ff44'),guide='none') +
theme_linedraw() + theme(legend.position = "none", aspect.ratio=1) +
ggtitle(sprintf('Gain-Only Decisions: CLASE%03g',subjIDs[subject]));
print(binary_gainonly_plot);
fig_name = sprintf('gainonly_CLASE%03g.png',subjIDs[subject]);
if (!file.exists(fig_name)){
ggsave(fig_name,height=4.2,width=4.6,dpi=300);
}
cat(sprintf('Subject %03i: missed %i trials; %.2f likelihood of correctly answering %g check trials; mean choice likelihood of %.2f given best estimates.\n',subjIDs[subject],0+sum((data$subjID == subjIDs[subject]) & is.na(data$choice)), likelihood_correct_check_trial[subject], number_check_trials,mean_choice_likelihood[subject]))
}
estimated_parameters = cbind(subjIDs, estimated_parameters)
data = as.data.frame(matrix(data = NA, nrow = 0, ncol = 14))
for(i in 1:number_of_subjects){
tmpf = read.csv(fn[i]);
data = rbind(data,tmpf)
}
subjIDs = unique(data$subjID);
likelihood_correct_check_trial = array(dim = c(number_of_subjects,1));
#### Initialize estimation procedure ####
set.seed(Sys.time()); #
number_of_parameters = 2;
initial_values_lowerbound = c(0.6, 25);
initial_values_upperbound = c(1.4, 100) - initial_values_lowerbound;
estimation_lowerbound = c(eps,eps); # lower bound of parameter values is machine precision above zero
estimation_upperbound = c(2, 300); # sensible/probable upper bounds on parameter values
# Create placeholders for the final estimates of the parameters, errors, and NLLs
estimated_parameters_restricted = array(dim = c(number_of_subjects, number_of_parameters),
dimnames = list(c(), c('rho','mu')));
estimated_parameter_errors_restricted = array(dim = c(number_of_subjects, number_of_parameters),
dimnames = list(c(), c('rho','mu')));
estimated_nlls_restricted = array(dim = c(number_of_subjects,1));
mean_choice_likelihood_restricted = array(dim = c(number_of_subjects,1));
mean_reactiontimes = array(dim = c(number_of_subjects,1));
mean_riskychoices = array(dim = c(number_of_subjects,1));
# Set up the parallelization
n.cores <- parallel::detectCores() - 1; # Use 1 less than the full number of cores.
my.cluster <- parallel::makeCluster(
n.cores,
type = "FORK"
)
doParallel::registerDoParallel(cl = my.cluster)
estimation_start_time = proc.time()[[3]]; # Start the clock
for (subject in 1:number_of_subjects){
subject_data = data[data$subjID == subjIDs[subject],];
finite_ind = is.finite(subject_data$choice);
tmpdata <- subject_data[finite_ind,]; # remove rows with NAs (missed choices) from estimation
choiceset = tmpdata[, 1:3];
choices = tmpdata$choice;
number_check_trials = sum(tmpdata$ischecktrial);
likelihood_correct_check_trial[subject] = check_trial_analysis(tmpdata);
mean_reactiontimes[subject] = mean(tmpdata$RT);
mean_riskychoices[subject] = mean(tmpdata$choice);
# Placeholders for all the iterations of estimation we're doing
all_estimates = matrix(nrow = iterations_per_estimation, ncol = number_of_parameters);
all_nlls = matrix(nrow = iterations_per_estimation, ncol = 1);
all_hessians = array(dim = c(iterations_per_estimation, number_of_parameters, number_of_parameters))
# The parallelized loop
alloutput <- foreach(iteration=1:iterations_per_estimation, .combine=rbind) %dopar% {
initial_values = runif(2)*initial_values_upperbound + initial_values_lowerbound; # create random initial values
# The estimation itself
output <- optim(initial_values, negLLprospect_lambda1, choiceset = choiceset, choices = choices,
method = "L-BFGS-B", lower = estimation_lowerbound, upper = estimation_upperbound, hessian = TRUE);
c(output$par,output$value); # the things (parameter values & NLL) to save/combine across parallel estimations
}
all_estimates = alloutput[,1:2];
all_nlls = alloutput[,3];
best_nll_index = which.min(all_nlls); # identify the single best estimation
# Save out the parameters & NLLs from the single best estimation
estimated_parameters_restricted[subject,] = all_estimates[best_nll_index,];
estimated_nlls_restricted[subject] = all_nlls[best_nll_index];
# Calculate & store the mean choice likelihood given our best estimates
choiceP = choice_probability_lambda1(choiceset,estimated_parameters[subject,]);
mean_choice_likelihood_restricted[subject] = mean(choices * choiceP + (1 - choices) * (1-choiceP));
# Calculate the hessian at those parameter values & save out
best_hessian = hessian(func=negLLprospect_lambda1, x = all_estimates[best_nll_index,], choiceset = choiceset, choices = choices)
estimated_parameter_errors_restricted[subject,] = sqrt(diag(solve(best_hessian)));
cat(sprintf('Subject %03i: missed %i trials; %.2f likelihood of correctly answering %g check trials; mean choice likelihood of %.2f given best estimates.\n',subjIDs[subject],0+sum((data$subjID == subjIDs[subject]) & is.na(data$choice)), likelihood_correct_check_trial[subject], number_check_trials,mean_choice_likelihood[subject]))
}
estimated_parameters_restricted = cbind(subjIDs, estimated_parameters_restricted)
LR_D = 2 * (estimated_nlls_restricted - estimated_nlls)
LR_D = (estimated_nlls_restricted - estimated_nlls)
LR_p = pchisq(LR_D, df=1, lower.tail=FALSE)
results_df <- data.frame(
subjID = subjIDs,
# Full model
nll_full = as.numeric(estimated_nlls),
rho_full = estimated_parameters[,"rho"],
lambda_full = estimated_parameters[,"lambda"],
mu_full = estimated_parameters[,"mu"],
# Restricted model
nll_restricted = as.numeric(estimated_nlls_restricted),
rho_restricted = estimated_parameters_restricted[,"rho"],
mu_restricted = estimated_parameters_restricted[,"mu"],
# LRT results
LR_D = as.numeric(LR_D),
LR_p = as.numeric(LR_p)
)
View(results_df)
write.csv(results_df, "model_comparison_results22.csv", row.names=FALSE)
estimated_parameters
# Means & SEs
colMeans(estimated_parameters[keepsubj,2:4])
colMeans(estimated_parameters[,2:4])
apply(estimated_parameters[,2:4], 2, sd)/sqrt(sum(22))
estimated_parameter_errors = cbind(subjIDs, estimated_parameter_errors)
estimated_parameter_errors
estimated_parameters
results_df <- data.frame(
subjID = subjIDs,
# Full model
nll_full = as.numeric(estimated_nlls),
rho_full = estimated_parameters[,"rho"],
lambda_full = estimated_parameters[,"lambda"],
mu_full = estimated_parameters[,"mu"],
rho_se = estimated_parameter_errors[,"rho"],
lambda_se = estimated_parameter_errors[,"lambda"],
mu_se = estimated_parameter_errors[,"mu"],
# Restricted model
nll_restricted = as.numeric(estimated_nlls_restricted),
rho_restricted = estimated_parameters_restricted[,"rho"],
mu_restricted = estimated_parameters_restricted[,"mu"],
# LRT results
LR_D = as.numeric(LR_D),
LR_p = as.numeric(LR_p)
)
results_df$AIC_full = 2*3 + 2*results_df$nll_full
results_df$AIC_restricted = 2*2 + 2*results_df$nll_restricted
results_df$BIC_full = log(135)*3 + 2*results_df$nll_full
results_df$BIC_restricted = log(135)*2 + 2*results_df$nll_restricted
subjIDs_gainseeking = test_results$subjIDs[test_results$lambda < 1 & test_results$isdifffrom1 == 1]
subjIDs_gainseeking = results_df$subjIDs[results_df$lambda < 1 & results_df$isdifffrom1 == 1]
subjIDs_gainseeking
test_results$isdifffrom1 = ifelse(df$LR_p < 0.05, 1, 0)
test_results$isdifffrom1 = ifelse(test_results$LR_p < 0.05, 1, 0)
results_df$isdifffrom1 = ifelse(results_df$LR_p < 0.05, 1, 0)
1.257332e-01
subjIDs_gainseeking = results_df$subjIDs[results_df$lambda < 1 & results_df$isdifffrom1 == 1]
subjIDs_gainlossneutral = test_results$subjIDs[test_results$isdifffrom1 == 0]
subjIDs_lossaverse = test_results$subjIDs[test_results$lambda > 1 & test_results$isdifffrom1 == 1]
subjIDs_gainseeking
results_df$lambda < 1
subjIDs_gainseeking = results_df$subjIDs[results_df$lambda_full < 1 & results_df$isdifffrom1 == 1]
subjIDs_gainseeking
results_df$lambda_full < 1
results_df$isdifffrom1 == 1
results_df$lambda_full < 1 & results_df$isdifffrom1 == 1
results_df$subjIDs[results_df$lambda_full < 1 & results_df$isdifffrom1 == 1]
subjIDs_gainseeking = results_df$subjIDs[results_df$lambda_full < 1 & results_df$isdifffrom1 == 1,]
subjIDs_gainseeking
results_df$subjIDs[results_df$lambda_full < 1 & results_df$isdifffrom1 == 1,]
results_df$subjIDs[,results_df$lambda_full < 1 & results_df$isdifffrom1 == 1]
results_df$subjID[results_df$lambda_full < 1 & results_df$isdifffrom1 == 1]
subjIDs_gainlossneutral = results_df$subjID[results_df$isdifffrom1 == 0]
results_df$subjID[results_df$isdifffrom1 == 0]
subjIDs_gainlossneutral = results_df$subjID[results_df$isdifffrom1 == 0]
subjIDs_lossaverse = results_df$subjIDs[results_df$lambda > 1 & results_df$isdifffrom1 == 1]
subjIDs_lossaverse
subjIDs_gainlossneutral = results_df$subjID[results_df$isdifffrom1 == 0]
subjIDs_lossaverse = results_df$subjIDs[results_df$lambda_full > 1 & results_df$isdifffrom1 == 1]
subjIDs_lossaverse
subjIDs_gainlossneutral = results_df$subjID[results_df$isdifffrom1 == 0]
subjIDs_lossaverse = results_df$subjID[results_df$lambda_full > 1 & results_df$isdifffrom1 == 1]
subjIDs_lossaverse
results_df$LAtype <- NA_character_
df$label[subjIDs_gainseeking] <- "GS_High"
results_df$LAtype <- NA_character_
results_df$label[subjIDs_gainseeking] <- "GS_High"
results_df$LAtype <- NA_character_
results_df$LAtype[subjIDs_gainseeking] <- "GS_High"
results_df$LAtype <- NA_character_
results_df$LAtype[subjIDs_gainseeking] <- "GS_High"
results_df$LAtype[subjIDs_gainlossneutral] <- "GLN_Neutral"
results_df$LAtype[subjIDs_lossaverse] <- "LA_Low"
subjIDs_lossaverse
subjIDs_gainseeking = results_df$lambda_full < 1 & results_df$isdifffrom1 == 1
subjIDs_gainlossneutral = results_df$isdifffrom1 == 0
subjIDs_lossaverse = results_df$lambda_full > 1 & results_df$isdifffrom1 == 1
results_df$LAtype <- NA_character_
results_df$LAtype[subjIDs_gainseeking] <- "GS_High"
results_df$LAtype[subjIDs_gainlossneutral] <- "GLN_Neutral"
results_df$LAtype[subjIDs_lossaverse] <- "LA_Low"
write.csv(results_df, "model_comparison_results23.csv", row.names=FALSE)
